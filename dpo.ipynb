{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install trl==0.15.2 peft bitsandbytes unsloth\n",
    "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T19:54:34.017114Z",
     "iopub.status.busy": "2025-03-09T19:54:34.016744Z",
     "iopub.status.idle": "2025-03-09T19:54:48.132241Z",
     "shell.execute_reply": "2025-03-09T19:54:48.131361Z",
     "shell.execute_reply.started": "2025-03-09T19:54:34.017059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import PatchDPOTrainer, FastLanguageModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "import torch\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "sft_model_path = \"../input/qwen2.5-0.5b-sft-20q/transformers/default/1\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    sft_model_path,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set up quantization config for efficient training\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load SFT model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        sft_model_path,\n",
    "        torch_dtype=torch.float16,  # Use float16 for efficiency\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \n",
    "        \"k_proj\", \n",
    "        \"v_proj\", \n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        param.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T19:54:48.133303Z",
     "iopub.status.busy": "2025-03-09T19:54:48.133056Z",
     "iopub.status.idle": "2025-03-09T19:54:55.279575Z",
     "shell.execute_reply": "2025-03-09T19:54:55.278816Z",
     "shell.execute_reply.started": "2025-03-09T19:54:48.133282Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "unsloth/qwen2.5-0.5b-unsloth-bnb-4bit does not have a padding token! Will use pad_token = <|vision_pad|>.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.9 patched 24 layers with 24 QKV layers, 24 O layers and 24 MLP layers.\n",
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "sft_model_path = \"../input/qwen2.5-0.5b-sft-20q/transformers/default/1\"\n",
    "\n",
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = sft_model_path, # Choose ANY! eg mistralai/Mistral-7B-Instruct-v0.2\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r =16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Currently only supports dropout = 0\n",
    "    bias = \"none\",    # Currently only supports bias = \"none\"\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T19:54:55.281383Z",
     "iopub.status.busy": "2025-03-09T19:54:55.281134Z",
     "iopub.status.idle": "2025-03-09T19:54:55.284722Z",
     "shell.execute_reply": "2025-03-09T19:54:55.284049Z",
     "shell.execute_reply.started": "2025-03-09T19:54:55.281363Z"
    }
   },
   "outputs": [],
   "source": [
    "PatchDPOTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"qwen-2.5\",\n",
    "# )\n",
    "# tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "with open(\"../input/preference-20q/preference_pairs.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# train_examples = []\n",
    "# for examples in data:\n",
    "#     # Apply chat template\n",
    "#     train_example = {\n",
    "#         \"prompt\": tokenizer.apply_chat_template(\n",
    "#             examples[\"prompt\"], \n",
    "#             tokenize=False, \n",
    "#             add_generation_prompt=False\n",
    "#         ) + tokenizer.eos_token,\n",
    "#         \"chosen\": tokenizer.apply_chat_template(\n",
    "#             examples[\"chosen\"], \n",
    "#             tokenize=False, \n",
    "#             add_generation_prompt=False\n",
    "#         ) + tokenizer.eos_token,\n",
    "#         \"rejected\": tokenizer.apply_chat_template(\n",
    "#             examples[\"rejected\"], \n",
    "#             tokenize=False, \n",
    "#             add_generation_prompt=False\n",
    "#         ) + tokenizer.eos_token,\n",
    "#     }\n",
    "#     train_examples.append(train_example)\n",
    "    \n",
    "\n",
    "# dataset = Dataset.from_list(train_examples).train_test_split(test_size=0.1)\n",
    "dataset = Dataset.from_list(data).train_test_split(test_size=0.1)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "print(f\"Created dataset with {len(train_dataset)} train examples and {len(test_dataset)} test examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T19:54:55.285981Z",
     "iopub.status.busy": "2025-03-09T19:54:55.285686Z",
     "iopub.status.idle": "2025-03-09T19:55:24.164111Z",
     "shell.execute_reply": "2025-03-09T19:55:24.163290Z",
     "shell.execute_reply.started": "2025-03-09T19:54:55.285949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dbcc56a5e942198eec9e1e6a4e6316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/30037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c2920776774e3b9fae4a0781c9fe39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=3):   0%|          | 0/929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "with open(\"../input/preference-20q/preference_pairs.json\", 'r') as f:\n",
    "    data = json.load(f)\n",
    "dataset = Dataset.from_list(data).train_test_split(test_size=0.03)\n",
    "column_names = list(dataset['train'].features)\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen-2.5\",\n",
    "    map_eos_token = True\n",
    ")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "def apply_dpo_template(example, tokenizer):\n",
    "  if all(k in example.keys() for k in (\"prompt\", \"chosen\", \"rejected\")):\n",
    "    # For DPO, the inputs are triples of (prompt, chosen, rejected), where `chosen` and `rejected` are the final turn of a dialogue\n",
    "    # We therefore need to extract the N-1 turns to form the prompt\n",
    "    prompt_messages = example[\"prompt\"]\n",
    "\n",
    "\n",
    "    # Now we extract the final turn to define chosen/rejected responses\n",
    "    chosen_messages = example[\"chosen\"]\n",
    "    rejected_messages = example[\"rejected\"]\n",
    "    example[\"text_chosen\"] = tokenizer.apply_chat_template(chosen_messages, tokenize=False)\n",
    "    example[\"text_rejected\"] = tokenizer.apply_chat_template(rejected_messages, tokenize=False)\n",
    "    example[\"text_prompt\"] = tokenizer.apply_chat_template(prompt_messages, tokenize=False)\n",
    "  return example\n",
    "\n",
    "dataset = dataset.map(\n",
    "    apply_dpo_template,\n",
    "    fn_kwargs = {\"tokenizer\": tokenizer},\n",
    "    num_proc = 3,\n",
    "    remove_columns = column_names,\n",
    ")\n",
    "\n",
    "for split in [\"train\", \"test\"]:\n",
    "    dataset[split] = dataset[split].rename_columns(\n",
    "        {\"text_prompt\": \"prompt\", \"text_chosen\": \"chosen\", \"text_rejected\": \"rejected\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T19:55:24.165603Z",
     "iopub.status.busy": "2025-03-09T19:55:24.165260Z",
     "iopub.status.idle": "2025-03-09T19:56:52.813937Z",
     "shell.execute_reply": "2025-03-09T19:56:52.813173Z",
     "shell.execute_reply.started": "2025-03-09T19:55:24.165576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf4454128dd459b90659dc6914d0fcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset (num_proc=3):   0%|          | 0/30037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44a4025a0ee14094895085f2b7ddce6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=3):   0%|          | 0/30037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e4f34ce39d46eda3363121cb977489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=3):   0%|          | 0/30037 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9b650c3f67d4e1b9349606b6c7218c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in eval dataset (num_proc=3):   0%|          | 0/929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39416cb76b7c4d61b00f10b67a2cbe85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to eval dataset (num_proc=3):   0%|          | 0/929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec663f6eca140ae8c9258152e98277f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing eval dataset (num_proc=3):   0%|          | 0/929 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = DPOConfig(\n",
    "    output_dir=\"qwen-20q-dpo\",\n",
    "    max_steps = 300,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-6, \n",
    "    max_length = 1024,\n",
    "    max_prompt_length=512,\n",
    "    warmup_ratio = 0.1,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    save_total_limit=3,\n",
    "    optim = \"adamw_8bit\",\n",
    "    lr_scheduler_type = \"linear\",\n",
    "    eval_strategy = 'steps',\n",
    "    eval_steps = 100,\n",
    "    seed = 3407,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    dataset_num_proc = 3,\n",
    "    dataloader_num_workers = 5,\n",
    "    beta = 0.1\n",
    ")\n",
    "\n",
    "# Create DPOTrainer\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    processing_class=tokenizer,\n",
    "    ref_model = None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T19:56:52.815348Z",
     "iopub.status.busy": "2025-03-09T19:56:52.814983Z",
     "iopub.status.idle": "2025-03-09T20:16:01.058871Z",
     "shell.execute_reply": "2025-03-09T20:16:01.057890Z",
     "shell.execute_reply.started": "2025-03-09T19:56:52.815312Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 30,037 | Num Epochs = 1 | Total steps = 300\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 1 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 8,798,208/345,364,352 (2.55% trained)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 18:59, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>eval_logits / chosen</th>\n",
       "      <th>eval_logits / rejected</th>\n",
       "      <th>nll_loss</th>\n",
       "      <th>aux_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.039686</td>\n",
       "      <td>-8.066851</td>\n",
       "      <td>-24.421848</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>16.354996</td>\n",
       "      <td>-791.979614</td>\n",
       "      <td>-1321.390503</td>\n",
       "      <td>-0.618758</td>\n",
       "      <td>-0.580438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.038416</td>\n",
       "      <td>-13.311496</td>\n",
       "      <td>-36.401848</td>\n",
       "      <td>0.987097</td>\n",
       "      <td>23.090347</td>\n",
       "      <td>-844.426086</td>\n",
       "      <td>-1441.190430</td>\n",
       "      <td>-0.580215</td>\n",
       "      <td>-0.546839</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.026356</td>\n",
       "      <td>-13.458762</td>\n",
       "      <td>-36.367233</td>\n",
       "      <td>0.991398</td>\n",
       "      <td>22.908474</td>\n",
       "      <td>-845.898743</td>\n",
       "      <td>-1440.844360</td>\n",
       "      <td>-0.579547</td>\n",
       "      <td>-0.546591</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.8780336210461489, metrics={'train_runtime': 1145.2461, 'train_samples_per_second': 1.048, 'train_steps_per_second': 0.262, 'total_flos': 0.0, 'train_loss': 0.8780336210461489, 'epoch': 0.03994673768308921})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:17:10.818876Z",
     "iopub.status.busy": "2025-03-09T20:17:10.817279Z",
     "iopub.status.idle": "2025-03-09T20:17:11.317273Z",
     "shell.execute_reply": "2025-03-09T20:17:11.316495Z",
     "shell.execute_reply.started": "2025-03-09T20:17:10.818834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dpo_model_20q/tokenizer_config.json',\n",
       " 'dpo_model_20q/special_tokens_map.json',\n",
       " 'dpo_model_20q/vocab.json',\n",
       " 'dpo_model_20q/merges.txt',\n",
       " 'dpo_model_20q/added_tokens.json',\n",
       " 'dpo_model_20q/tokenizer.json')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"dpo_model_20q\")\n",
    "tokenizer.save_pretrained(\"dpo_model_20q\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:17:14.024251Z",
     "iopub.status.busy": "2025-03-09T20:17:14.023900Z",
     "iopub.status.idle": "2025-03-09T20:17:14.028672Z",
     "shell.execute_reply": "2025-03-09T20:17:14.027780Z",
     "shell.execute_reply.started": "2025-03-09T20:17:14.024226Z"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth import FastLanguageModel\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "max_seq_length=2048\n",
    "dtype=None\n",
    "load_in_4bit=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-09T20:18:36.084655Z",
     "iopub.status.busy": "2025-03-09T20:18:36.084259Z",
     "iopub.status.idle": "2025-03-09T20:18:54.083202Z",
     "shell.execute_reply": "2025-03-09T20:18:54.082518Z",
     "shell.execute_reply.started": "2025-03-09T20:18:36.084622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.9: Fast Qwen2 patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2755d9ea540424bb800fed92fc9ae8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f988692b07d4e2bb8efe847acec121d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/171 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the object alive?ï¿½\n",
      "ï¿½user\n",
      "No.ï¿½\n",
      "ï¿½assistant\n",
      "Is the object man-made?ï¿½\n",
      "ï¿½user\n",
      "Yes.ï¿½\n",
      "ï¿½assistant\n",
      "Is the object a tool?ï¿½\n",
      "ï¿½assistant\n",
      "Yes.ï¿½\n",
      "ï¿½assistant\n",
      "Is the object a machine?ï¿½\n",
      "ï¿½assistant\n",
      "Yes.ï¿½\n",
      "ï¿½assistant\n",
      "Is the object a computer?ï¿½\n",
      "ï¿½assistant\n",
      "No.ï¿½\n",
      "ï¿½assistant\n",
      "Is the object a vehicle?ï¿½\n",
      "ï¿½assistant\n",
      "Yes.ï¿½\n",
      "ï¿½assistant\n",
      "Is the object a car?ï¿½\n",
      "ï¿½assistant\n",
      "No.ï¿½\n",
      "ï¿½assistant\n",
      "Is the object\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"../working/dpo_model_20q\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"You are playing 20 questions to guess a keyword. You are playing the role of the Questioner. Please ask a yes-or-no question and try to guess the keyword.\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128,\n",
    "                   use_cache = True, temperature = 1.5, min_p = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6824594,
     "sourceId": 10968545,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 261168,
     "modelInstanceId": 239518,
     "sourceId": 279605,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
