{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10956805,"sourceType":"datasetVersion","datasetId":6816296}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"! pip install trl==0.15.2 peft unsloth\n!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\nimport json\nimport torch\nfrom datasets import Dataset\nfrom transformers import DataCollatorForLanguageModeling","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T08:39:48.919205Z","iopub.execute_input":"2025-03-08T08:39:48.919511Z","iopub.status.idle":"2025-03-08T08:40:02.557678Z","shell.execute_reply.started":"2025-03-08T08:39:48.919489Z","shell.execute_reply":"2025-03-08T08:40:02.556246Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\nðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Initialize model and tokenizer\nmodel_name = \"Qwen/Qwen2.5-0.5B\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    trust_remote_code=True,\n)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,  # Use float16 for efficiency\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\n\n# Configure model for training\nmodel.config.use_cache = False  # Disable KV caching for training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T08:40:02.581198Z","iopub.execute_input":"2025-03-08T08:40:02.581476Z","iopub.status.idle":"2025-03-08T08:40:04.987947Z","shell.execute_reply.started":"2025-03-08T08:40:02.581449Z","shell.execute_reply":"2025-03-08T08:40:04.987250Z"}},"outputs":[{"name":"stderr","text":"Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# max_seq_length = 2048\n# dtype = None  # Auto-detect\n# load_in_4bit = True\n\n# model, tokenizer = FastLanguageModel.from_pretrained(\n#     model_name = \"Qwen/Qwen2.5-0.5B\",\n#     max_seq_length = max_seq_length,\n#     dtype = dtype,\n#     load_in_4bit = load_in_4bit,\n# )\n\n# model = FastLanguageModel.get_peft_model(\n#     model,\n#     r = 16,\n#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n#                        \"gate_proj\", \"up_proj\", \"down_proj\",],\n#     lora_alpha = 16,\n#     lora_dropout = 0,\n#     bias = \"none\",\n#     use_gradient_checkpointing = \"unsloth\",\n#     random_state = 3407,\n# )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_20q_text_dataset(json_path, tokenizer):\n    \"\"\"\n    Prepare 20 Questions dataset as plain text input-output pairs.\n    \"\"\"\n    # Load the JSON data\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    train_examples = []\n    \n    for example in data:\n        lines = example[\"lines\"]\n        target_word = example[\"word\"][0] if example[\"word\"] else \"\"\n        correct = example[\"correct\"]\n        \n        # Build conversation history\n        history = []\n        history.append({\"role\": \"user\", \"content\": \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask a yes-or-no question and try to guess the keyword.\"})\n            \n        # Process each conversation into training examples\n        for i in range(0, len(lines)):\n            sentence = lines[i].split(\"? \")\n            if len(sentence) < 2:\n                continue\n            question = sentence[0] + \"?\"\n            answer = sentence[1]\n            \n            # Add current question as assistant message\n            history.append({\"role\": \"assistant\", \"content\": question})\n            # Add current answer as user message\n            if i+1 == len(lines):\n                if correct:\n                    history.append({\"role\": \"user\", \"content\": f\"{answer}! You guess the right answer ({target_word})\"})\n                else:\n                    history.append({\"role\": \"user\", \"content\": f\"{answer}! You lose. Better luck next time!\"})\n            else:\n                history.append({\"role\": \"user\", \"content\": answer})\n        # Apply chat template\n        train_example = {\n            \"text\": tokenizer.apply_chat_template(\n                history, \n                tokenize=False, \n                add_generation_prompt=False\n            ) + tokenizer.eos_token\n        }\n        \n        train_examples.append(train_example)\n    \n    # Convert to Dataset format\n    return Dataset.from_list(train_examples)\n\n# Create training dataset\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"qwen-2.5\",\n)\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\ntrain_dataset = prepare_20q_text_dataset(\"../input/raw-20q/train.json\", tokenizer)\n# train_dataset = train_dataset.train_test_split(test_size=0.1)\nprint(f\"Created dataset with {len(train_dataset)} examples\")\n\n# data_collator = DataCollatorForLanguageModeling(\n#     tokenizer=tokenizer,\n#     mlm=False  # We're not doing masked language modeling\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T08:58:55.059535Z","iopub.execute_input":"2025-03-08T08:58:55.059913Z","iopub.status.idle":"2025-03-08T08:59:19.371072Z","shell.execute_reply.started":"2025-03-08T08:58:55.059884Z","shell.execute_reply":"2025-03-08T08:59:19.370255Z"}},"outputs":[{"name":"stdout","text":"Created dataset with 100000 examples\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# from trl import SFTTrainer\n# from transformers import TrainingArguments\n# from unsloth import is_bfloat16_supported\n\n# trainer = SFTTrainer(\n#     model = model,\n#     tokenizer = tokenizer,\n#     train_dataset = train_dataset,\n#     dataset_text_field = \"text\",\n#     max_seq_length = max_seq_length,\n#     dataset_num_proc = 3,\n#     packing = False, # Can make training 5x faster for short sequences.\n#     args = TrainingArguments(\n#         per_device_train_batch_size = 2,\n#         gradient_accumulation_steps = 4,\n#         warmup_steps = 5,\n#         # num_train_epochs = 1, # Set this for 1 full training run.\n#         max_steps = 1000,\n#         learning_rate = 2e-4,\n#         fp16 = not is_bfloat16_supported(),\n#         bf16 = is_bfloat16_supported(),\n#         logging_steps = 1,\n#         optim = \"adamw_8bit\",\n#         weight_decay = 0.01,\n#         lr_scheduler_type = \"linear\",\n#         seed = 3407,\n#         output_dir = \"outputs\",\n#         report_to = \"none\", # Use this for WandB etc\n#     ),\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T08:55:22.951269Z","iopub.execute_input":"2025-03-08T08:55:22.951644Z","iopub.status.idle":"2025-03-08T08:55:23.062556Z","shell.execute_reply.started":"2025-03-08T08:55:22.951611Z","shell.execute_reply":"2025-03-08T08:55:23.061358Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-a539774d6253>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mdataset_text_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mdataset_num_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpacking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# Can make training 5x faster for short sequences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'max_seq_length' is not defined"],"ename":"NameError","evalue":"name 'max_seq_length' is not defined","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"from trl import SFTTrainer, SFTConfig\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom peft import LoraConfig, get_peft_model\n\n# Define LoRA configuration for parameter-efficient fine-tuning\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n)\n\n# Apply LoRA to model\nmodel = get_peft_model(model, peft_config)\n\n# Define training arguments\ntraining_args = SFTConfig(\n    output_dir=\"qwen-20q-sft\",\n    max_steps = 1000,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=4,\n    learning_rate=2e-4, \n    warmup_steps = 5,\n    weight_decay=0.01,\n    logging_steps=10,\n    save_steps=200,\n    save_total_limit=3,\n    optim = \"adamw_8bit\",\n    lr_scheduler_type = \"linear\",\n    seed = 3407,\n    fp16=True,\n    report_to=\"none\",\n    dataset_num_proc = 3,\n    max_seq_length=2048\n)\n\n# Create SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    processing_class=tokenizer,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:01:26.000645Z","iopub.execute_input":"2025-03-08T09:01:26.001135Z","iopub.status.idle":"2025-03-08T09:02:35.744926Z","shell.execute_reply.started":"2025-03-08T09:01:26.001092Z","shell.execute_reply":"2025-03-08T09:02:35.744215Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Tokenizing to [\"text\"] (num_proc=3):   0%|          | 0/100000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6f137bf46441178d4b3b85fddc724a"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:02:39.811518Z","iopub.execute_input":"2025-03-08T09:02:39.811862Z","iopub.status.idle":"2025-03-08T09:19:20.622440Z","shell.execute_reply.started":"2025-03-08T09:02:39.811837Z","shell.execute_reply":"2025-03-08T09:19:20.621402Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 16:38, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.632000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.902400</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.751100</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.728800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.701100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.701200</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.688900</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.672800</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.680200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.669700</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.665300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.662700</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.658100</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.677100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.648400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.683100</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.647800</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.662200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.665200</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.660000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.650300</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.642000</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.662600</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.649200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.667800</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.647900</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.670500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.635600</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.651700</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.650000</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.651000</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.657800</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.662900</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.660000</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.645300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.641300</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.637100</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.639200</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.647900</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.644800</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.642400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.627000</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.641700</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.653300</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.641000</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.648900</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.641200</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.636900</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.645700</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.645500</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.656100</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.655600</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.655100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.671000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.637400</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.634900</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.648700</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>1.647900</td>\n    </tr>\n    <tr>\n      <td>590</td>\n      <td>1.651100</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.630500</td>\n    </tr>\n    <tr>\n      <td>610</td>\n      <td>1.625000</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>1.645000</td>\n    </tr>\n    <tr>\n      <td>630</td>\n      <td>1.630200</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>1.646400</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>1.626200</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>1.634400</td>\n    </tr>\n    <tr>\n      <td>670</td>\n      <td>1.605100</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>1.647600</td>\n    </tr>\n    <tr>\n      <td>690</td>\n      <td>1.636300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.645200</td>\n    </tr>\n    <tr>\n      <td>710</td>\n      <td>1.635700</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>1.630100</td>\n    </tr>\n    <tr>\n      <td>730</td>\n      <td>1.639300</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>1.631700</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>1.632100</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>1.636600</td>\n    </tr>\n    <tr>\n      <td>770</td>\n      <td>1.629800</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>1.619700</td>\n    </tr>\n    <tr>\n      <td>790</td>\n      <td>1.622900</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.613000</td>\n    </tr>\n    <tr>\n      <td>810</td>\n      <td>1.653500</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>1.648400</td>\n    </tr>\n    <tr>\n      <td>830</td>\n      <td>1.630200</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>1.638400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>1.629700</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>1.649500</td>\n    </tr>\n    <tr>\n      <td>870</td>\n      <td>1.635600</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>1.624800</td>\n    </tr>\n    <tr>\n      <td>890</td>\n      <td>1.626000</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.632100</td>\n    </tr>\n    <tr>\n      <td>910</td>\n      <td>1.653500</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>1.639800</td>\n    </tr>\n    <tr>\n      <td>930</td>\n      <td>1.640200</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>1.630500</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>1.659700</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>1.636400</td>\n    </tr>\n    <tr>\n      <td>970</td>\n      <td>1.630300</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>1.636100</td>\n    </tr>\n    <tr>\n      <td>990</td>\n      <td>1.634300</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.653800</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=1.6610012664794922, metrics={'train_runtime': 1000.165, 'train_samples_per_second': 7.999, 'train_steps_per_second': 1.0, 'total_flos': 8081271101583360.0, 'train_loss': 1.6610012664794922})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"model.save_pretrained(\"sft_model\")\ntokenizer.save_pretrained(\"sft_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T09:20:41.373117Z","iopub.execute_input":"2025-03-08T09:20:41.373488Z","iopub.status.idle":"2025-03-08T09:20:41.676216Z","shell.execute_reply.started":"2025-03-08T09:20:41.373459Z","shell.execute_reply":"2025-03-08T09:20:41.675307Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"('sft_model/tokenizer_config.json',\n 'sft_model/special_tokens_map.json',\n 'sft_model/vocab.json',\n 'sft_model/merges.txt',\n 'sft_model/added_tokens.json',\n 'sft_model/tokenizer.json')"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}